{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "### matplotlib 한글 폰트 설정 #############################\n",
    "from matplotlib import font_manager, rc\n",
    "## for window #####\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "## for mac #####\n",
    "#rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocessing for target\n",
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size/batch_size\n",
    "embedding_dim =32\n",
    "units = 128\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size=buffer_size)\n",
    "data = data.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid',\n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden,1)\n",
    "        \n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "        context_vector = attention_weights + enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector,1)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x], axis=-1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints2'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "#summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.0402 Batch Loss 1.0053\n",
      "Epoch 10 Loss 0.0343 Batch Loss 0.8572\n",
      "Epoch 20 Loss 0.0309 Batch Loss 0.7717\n",
      "Epoch 30 Loss 0.0251 Batch Loss 0.6278\n",
      "Epoch 40 Loss 0.0225 Batch Loss 0.5621\n",
      "Epoch 50 Loss 0.0204 Batch Loss 0.5110\n",
      "Epoch 60 Loss 0.0177 Batch Loss 0.4432\n",
      "Epoch 70 Loss 0.0142 Batch Loss 0.3559\n",
      "Epoch 80 Loss 0.0104 Batch Loss 0.2590\n",
      "Epoch 90 Loss 0.0069 Batch Loss 0.1731\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                \n",
    "                predictions, dec_hidden, _= decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20f444ad9a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I feel hungry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
    "#                                             s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize' : 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "    \n",
    "    print('Input : {}'.format(sentence))\n",
    "    print('Predicted translation : {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20f446a6fa0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : I feel hungry\n",
      "Predicted translation : 나는 배가 고프다 <eos> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'I feel hungry'\n",
    "\n",
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
